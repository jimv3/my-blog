{"componentChunkName":"component---src-templates-blog-post-js","path":"/confusion-matrix/","result":{"data":{"site":{"siteMetadata":{"title":"Jim Vaughan","siteUrl":"https://jimvaughan.dev"}},"markdownRemark":{"id":"08e09988-f23d-5473-ac8e-c35d23dba73d","excerpt":"TL;DR A confusion matrix is a way to check the performance of a binary classifier in machine learning. The matrix tracks the number of true positives, falseâ€¦","html":"<h3>TL;DR</h3>\n<p>A confusion matrix is a way to check the performance of a binary classifier in machine learning. The matrix tracks the number of true positives, false positives, true negatives, and false negatives. These values can be used to calculate other performance metrics.</p>\n<h3>What does the confusion matrix contain?</h3>\n<p>The confusion matrix is a two-by-two table where the actual classes are mapped against the predicted classes. In the case of true/false outcomes you will have a the true positives (predicted true and actually true), true negatives (predicted false and actually false), false positives (predicted true but actually false), and false negatives (predicted false but actually true). These pairings can then be used to calculate other performance metrics.</p>\n<h3>Accuracy</h3>\n<p>Accuracy, as you may have guessed, is how many of the classifications were made correctly. It simply tells us how many of the cases are accurate. But this metric in itself is not the only measure to look at for a classification model. For example, if your classifier simply returns true for every example in a sample size of 100, and there are 99 true cases in the sample, then your model is 99% accurate. No machine learning would even be necessary in that example.</p>\n<p>Accuracy can be calculated by adding true positives plus true negatives and dividing by total number of records. (TP + TN) / (TP + FP + TN + FN).</p>\n<h3>Precision</h3>\n<p>Precision is how well your model does at classifying the true values and only the true values. In other words, the score is negatively impacted by the number of times the model incorrectly places the true label on an instance.</p>\n<p>Precision can be calculated by dividing the true positives by the total number of predicted positives. TP / (TP + FP)</p>\n<h3>Recall</h3>\n<p>Recall describes how well your model finds all of the true values. In other words, the score is negatively impacted if the model labels a true value as negative.</p>\n<p>Recall can be calculated by dividing the true positives by the total number of actual positives. TP / (TP + FN)</p>","frontmatter":{"title":"Confusion Matrix and Its Uses","date":"April 11, 2020","description":"What is a confusion matrix and how is it useful."}}},"pageContext":{"slug":"/confusion-matrix/","previous":{"fields":{"slug":"/what-is-mase/"},"frontmatter":{"title":"What is MASE"}},"next":{"fields":{"slug":"/i-dont-have-enough-experience/"},"frontmatter":{"title":"I Don't Have Enough Experience"}}}},"staticQueryHashes":["2677188060","63159454"]}